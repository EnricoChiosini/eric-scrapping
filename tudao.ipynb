{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'temp' apagada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"temp\"):\n",
    "    shutil.rmtree(\"temp\")\n",
    "    print(f\"Pasta 'temp' apagada com sucesso.\")\n",
    "else:\n",
    "    print(\"Pasta 'temp' já não existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://eric.ed.gov/?q=interdisciplinary+OR+interdisciplinarys+OR+interdisciplinarity+OR+interdisciplinarities+OR+interdisciplinar+OR+interdisciplinares+OR+interdiscipline+OR+interdisciplines+OR+multidisciplinary+OR+multidisciplinarys+OR+multidisciplinarity+OR+multidisciplinarities+OR+multidisciplinar+OR+multidisciplinares+OR+multidiscipline+OR+multidisciplines+OR+pluridisciplinary+OR+pluridisciplinarys+OR+pluridisciplinarity+OR+pluridisciplinarities+OR+pluridisciplinar+OR+pluridisciplinares+OR+pluridiscipline+OR+pluridisciplines+OR+transdisciplinary+OR+transdisciplinarys+OR+transdisciplinarity+OR+transdisciplinarities+OR+transdisciplinar+OR+transdisciplinares+OR+transdiscipline+OR+transdisciplines&pr=on&ft=on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for page to load...\n",
      "Page loaded.\n",
      "Number of results: 4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 23/23 [00:03<00:00,  7.34file/s]\n"
     ]
    }
   ],
   "source": [
    "articles_per_page = 200\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), \"temp\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "options.set_preference(\"browser.download.folderList\", 2)\n",
    "options.set_preference(\"browser.download.dir\", save_dir)\n",
    "options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "waiter = WebDriverWait(driver, 20, 0.1)  # espera por 20 segundos, com intervalo de 0.1 segundos entre as tentativas\n",
    "action = ActionChains(driver) \n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "print('Waiting for page to load...')\n",
    "waiter.until(\n",
    "    lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    ")\n",
    "print('Page loaded.')\n",
    "\n",
    "div_text = driver.find_element(By.ID, \"rr0\").text.replace(',', '')\n",
    "match = re.search(r'(?:all\\s+)?(\\d+)\\s+results', div_text)\n",
    "num = int(match.group(1))\n",
    "print(f\"Number of results: {num}\")\n",
    "\n",
    "export_link = driver.find_element(By.XPATH, '//a[@onclick=\"if(document.getElementById(\\'divExportList\\').style.display==\\'block\\'){document.getElementById(\\'divExportList\\').style.display=\\'none\\';}else{document.getElementById(\\'divSaveList\\').style.display=\\'none\\';document.getElementById(\\'divExportList\\').style.display=\\'block\\';}return(false);\"]')\n",
    "action.move_to_element(export_link).click().perform()\n",
    "\n",
    "results_to_include = driver.find_element(By.ID, \"selectExport\")\n",
    "select = Select(results_to_include)\n",
    "select.select_by_value(str(articles_per_page))\n",
    "\n",
    "for start in tqdm(range(1, num + 1, articles_per_page),\n",
    "                  desc=\"Downloading files\",\n",
    "                  unit=\"file\"):\n",
    "    \n",
    "    input_field = driver.find_element(By.ID, \"inputExport\")\n",
    "    input_field.clear()\n",
    "    input_field.send_keys(str(start))\n",
    "    \n",
    "    button = driver.find_element(By.XPATH, '//input[@type=\"submit\" and @value=\"Create file\"]')\n",
    "    button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos concatenados com sucesso!\n",
      "Total de registros no DataFrame: 4466\n"
     ]
    }
   ],
   "source": [
    "def parse_nbib(file_path):\n",
    "    data = {\"TI\": [], \"AU\": [], \"DP\": [], \"JT\": [], \"LA\": [], \"LID\": [], \"AB\": []}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_entry = {key: \"\" for key in data.keys()}\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if any(current_entry.values()):\n",
    "                    for key in data.keys():\n",
    "                        data[key].append(current_entry[key].strip())\n",
    "                    current_entry = {key: \"\" for key in data.keys()}\n",
    "                continue\n",
    "            \n",
    "            for key in data.keys():\n",
    "                if line.startswith(key + \" - \") or line.startswith(key + \"  - \"):\n",
    "                    if key == \"AU\":\n",
    "                        current_entry[key] += line[len(key) + 3:].strip() + \"; \"\n",
    "                    else:\n",
    "                        current_entry[key] += line[len(key) + 3:].strip() + \" \"\n",
    "                    break\n",
    "    \n",
    "    if any(current_entry.values()):\n",
    "        for key in data.keys():\n",
    "            data[key].append(current_entry[key].strip())\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "concatenated_filename = 'concatenated.nbib'\n",
    "concatenated_path = os.path.join(save_dir, concatenated_filename)\n",
    "\n",
    "with open(concatenated_path, 'w', encoding='utf-8') as outfile:\n",
    "    for file_name in os.listdir(save_dir):\n",
    "        if file_name.endswith('.nbib') and file_name != concatenated_filename:\n",
    "            file_path = os.path.join(save_dir, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")  # Adiciona quebra de linha entre os arquivos\n",
    "\n",
    "for file_name in os.listdir(save_dir):\n",
    "    if file_name.endswith('.nbib') and file_name != concatenated_filename:\n",
    "        os.remove(os.path.join(save_dir, file_name))\n",
    "\n",
    "df = parse_nbib(concatenated_path)\n",
    "\n",
    "excel_output_path = os.path.join(save_dir, 'nbib_data.xlsx')\n",
    "df.to_excel(excel_output_path, index=False)\n",
    "\n",
    "csv_gz_output_path = os.path.join(save_dir, 'nbib_data.csv.gz')\n",
    "df.to_csv(csv_gz_output_path, index=False, compression='gzip')\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"Arquivos concatenados com sucesso!\")\n",
    "    print(f\"Total de registros no DataFrame: {len(df)}\")\n",
    "else:\n",
    "    print(\"Falha ao concatenar os arquivos. O DataFrame está vazio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de artigos filtrados: 341\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_gz_output_path, compression='gzip')\n",
    "\n",
    "palavras_or_2 = ['teach', 'educat', 'learn']\n",
    "palavras_or_1 = [\n",
    "    'physic',   # physics\n",
    "    'chem',     # chemistry\n",
    "    'biolog',   # biology\n",
    "    'astronom', # astronomy\n",
    "    'geolog',   # geology\n",
    "    'scien',    # science\n",
    "    'ecolog',   # ecology\n",
    "]\n",
    "\n",
    "def verifica_texto(row):\n",
    "    # Concatena TI e AB e converte para minúsculas\n",
    "    texto = (str(row['TI']) + \" \" + str(row['AB'])).lower()\n",
    "    tokens = texto.split()\n",
    "    # Percorre os tokens procurando por pares consecutivos em que um\n",
    "    # inicia com uma palavra de palavras_or_1 e o outro com uma de palavras_or_2,\n",
    "    # em qualquer ordem.\n",
    "    for i in range(len(tokens) - 1):\n",
    "        t1, t2 = tokens[i], tokens[i+1]\n",
    "        if (any(t1.startswith(p) for p in palavras_or_1) and \n",
    "            any(t2.startswith(p) for p in palavras_or_2)):\n",
    "            return True\n",
    "        if (any(t1.startswith(p) for p in palavras_or_2) and \n",
    "            any(t2.startswith(p) for p in palavras_or_1)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df = df.drop_duplicates(subset=['TI', 'AB'])\n",
    "\n",
    "df_filtrado = df[df.apply(verifica_texto, axis=1)]\n",
    "\n",
    "df_filtrado.to_csv(csv_gz_output_path, index=False, compression='gzip')\n",
    "df_filtrado.to_excel(excel_output_path, index=False)\n",
    "\n",
    "print(f\"Total de artigos filtrados: {len(df_filtrado)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de informações da busca e filtragem salvo!\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo de saída\n",
    "output_txt_path = os.path.join(save_dir, 'informacoes.txt')\n",
    "\n",
    "# Conteúdo do arquivo\n",
    "content = f\"\"\"\n",
    "URL: {url}\n",
    "Number of results: {num}\n",
    "Palavras OR 1: {', '.join(palavras_or_1)}\n",
    "Palavras OR 2: {', '.join(palavras_or_2)}\n",
    "Total de artigos filtrados: {len(df_filtrado)}\n",
    "\"\"\"\n",
    "\n",
    "# Salvando o arquivo\n",
    "with open(output_txt_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Arquivo de informações da busca e filtragem salvo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
