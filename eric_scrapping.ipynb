{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last time updated: 2023-10-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff imported ðŸ¤“\n"
     ]
    }
   ],
   "source": [
    "#import stuff\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"stuff imported ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp folder deleted ðŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# delete temp folder if exists\n",
    "\n",
    "if os.path.exists(\"temp\"):\n",
    "    shutil.rmtree(\"temp\")\n",
    "    print(f\"temp folder deleted ðŸ¤“\")\n",
    "else:\n",
    "    print(\"temp folder does not exist ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url defined ðŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# defines the search url on ERIC website, just copy and paste it here\n",
    "\n",
    "url = \"https://eric.ed.gov/?q=interdisciplinary&pr=on&ff1=dtySince_2024\"\n",
    "\n",
    "print(\"url defined ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for page to load...\n",
      "page loaded\n",
      "number of results: 1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.89file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download completed ðŸ¤“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# does the scrapping to download the .nbib files from ERIC website\n",
    "\n",
    "articles_per_page = 200\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), \"temp\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "options.set_preference(\"browser.download.folderList\", 2)\n",
    "options.set_preference(\"browser.download.dir\", save_dir)\n",
    "options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "waiter = WebDriverWait(driver, 20, 0.1)\n",
    "action = ActionChains(driver) \n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "print('waiting for page to load...')\n",
    "waiter.until(\n",
    "    lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    ")\n",
    "print('page loaded')\n",
    "\n",
    "div_text = driver.find_element(By.ID, \"rr0\").text.replace(',', '')\n",
    "match = re.search(r'(?:all\\s+)?(\\d+)\\s+results', div_text)\n",
    "num = int(match.group(1))\n",
    "print(f\"number of results: {num}\")\n",
    "\n",
    "export_link = driver.find_element(By.XPATH, '//a[@onclick=\"if(document.getElementById(\\'divExportList\\').style.display==\\'block\\'){document.getElementById(\\'divExportList\\').style.display=\\'none\\';}else{document.getElementById(\\'divSaveList\\').style.display=\\'none\\';document.getElementById(\\'divExportList\\').style.display=\\'block\\';}return(false);\"]')\n",
    "action.move_to_element(export_link).click().perform()\n",
    "\n",
    "results_to_include = driver.find_element(By.ID, \"selectExport\")\n",
    "select = Select(results_to_include)\n",
    "select.select_by_value(str(articles_per_page))\n",
    "\n",
    "for start in tqdm(range(1, num + 1, articles_per_page),\n",
    "                  desc=\"downloading files\",\n",
    "                  unit=\"file\"):\n",
    "    \n",
    "    input_field = driver.find_element(By.ID, \"inputExport\")\n",
    "    input_field.clear()\n",
    "    input_field.send_keys(str(start))\n",
    "    \n",
    "    button = driver.find_element(By.XPATH, '//input[@type=\"submit\" and @value=\"Create file\"]')\n",
    "    button.click()\n",
    "\n",
    "print(\"download completed ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".nbib files concatenated successfully, .xlsx and .csv.gz files created ðŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# concatenate all .nbib files into one file and create a .xlsx and .csv.gz with it\n",
    "\n",
    "def parse_nbib_all_fields(file_path):\n",
    "    entries = []\n",
    "    current_entry = defaultdict(str)\n",
    "    tag = None\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if current_entry:\n",
    "                    entries.append(dict(current_entry))\n",
    "                    current_entry = defaultdict(str)\n",
    "                    tag = None\n",
    "                continue\n",
    "\n",
    "            match = re.match(r\"^([A-Z]{2,4})\\s+-\\s+(.*)\", line)\n",
    "            if match:\n",
    "                tag, content = match.groups()\n",
    "                if tag in current_entry:\n",
    "                    current_entry[tag] += \"; \" + content.strip()\n",
    "                else:\n",
    "                    current_entry[tag] = content.strip()\n",
    "            else:\n",
    "                if tag:\n",
    "                    current_entry[tag] += \" \" + line.strip()\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append(dict(current_entry))\n",
    "\n",
    "    return pd.DataFrame(entries)\n",
    "\n",
    "concatenated_filename = 'concatenated.nbib'\n",
    "concatenated_path = os.path.join(save_dir, concatenated_filename)\n",
    "\n",
    "with open(concatenated_path, 'w', encoding='utf-8') as outfile:\n",
    "    for file_name in os.listdir(save_dir):\n",
    "        if file_name.endswith('.nbib') and file_name != concatenated_filename:\n",
    "            file_path = os.path.join(save_dir, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "for file_name in os.listdir(save_dir):\n",
    "    if file_name.endswith('.nbib') and file_name != concatenated_filename:\n",
    "        os.remove(os.path.join(save_dir, file_name))\n",
    "\n",
    "df = parse_nbib_all_fields(concatenated_path)\n",
    "\n",
    "excel_output_path = os.path.join(save_dir, 'nbib_data.xlsx')\n",
    "df.to_excel(excel_output_path, index=False)\n",
    "\n",
    "csv_gz_output_path = os.path.join(save_dir, 'nbib_data.csv.gz')\n",
    "df.to_csv(csv_gz_output_path, index=False, compression='gzip')\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"the .nbib files concatenated successfully, .xlsx and .csv.gz files created ðŸ¤“\")\n",
    "else:\n",
    "    print(\"failed to concatenate files, DataFrame is empty... probably... ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de artigos filtrados: 356\n"
     ]
    }
   ],
   "source": [
    "# searches and filters the articles based on keywords\n",
    "\n",
    "df = pd.read_csv(csv_gz_output_path, compression='gzip')\n",
    "\n",
    "palavras_or_2 = ['teach', 'educat', 'learn']\n",
    "palavras_or_1 = ['physic']\n",
    "\n",
    "def verifica_texto(row):\n",
    "    texto = (str(row['TI']) + \" \" + str(row['AB'])).lower()\n",
    "    tokens = texto.split()\n",
    "    for i in range(len(tokens) - 1):\n",
    "        t1, t2 = tokens[i], tokens[i+1]\n",
    "        if (any(t1.startswith(p) for p in palavras_or_1) and \n",
    "            any(t2.startswith(p) for p in palavras_or_2)):\n",
    "            return True\n",
    "        if (any(t1.startswith(p) for p in palavras_or_2) and \n",
    "            any(t2.startswith(p) for p in palavras_or_1)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df = df.drop_duplicates(subset=['TI', 'AB'])\n",
    "\n",
    "df_filtrado = df[df.apply(verifica_texto, axis=1)]\n",
    "\n",
    "df_filtrado.to_csv(csv_gz_output_path, index=False, compression='gzip')\n",
    "df_filtrado.to_excel(excel_output_path, index=False)\n",
    "\n",
    "print(f\"Total de artigos filtrados: {len(df_filtrado)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes a .txt file with the search and filtering information\n",
    "\n",
    "output_txt_path = os.path.join(save_dir, 'informacoes.txt')\n",
    "\n",
    "content = f\"\"\"\n",
    "URL: {url}\n",
    "Number of results: {num}\n",
    "Palavras OR 1: {', '.join(palavras_or_1)}\n",
    "Palavras OR 2: {', '.join(palavras_or_2)}\n",
    "Total de artigos filtrados: {len(df_filtrado)}\n",
    "\"\"\"\n",
    "\n",
    "with open(output_txt_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Arquivo de informaÃ§Ãµes da busca e filtragem salvo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
